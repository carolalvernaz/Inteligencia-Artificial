# -*- coding: utf-8 -*-
"""Backpropagation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rCHfwteNFBQkffJhVKHrMOCAFzlnXA--
"""

import numpy as np
from itertools import product

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivada(x):
    return x * (1 - x)

def gerar_dados(n, funcao):
    X = np.array(list(product([0, 1], repeat=n)))
    if funcao == 'AND':
        y = np.array([[int(np.all(xi))] for xi in X])
    elif funcao == 'OR':
        y = np.array([[int(np.any(xi))] for xi in X])
    elif funcao == 'XOR':
        y = np.array([[int(np.sum(xi) % 2)] for xi in X])
    else:
        raise ValueError("Função lógica inválida. Use AND, OR ou XOR.")
    return X, y

class MLP:
    def __init__(self, n_entradas, n_oculta=None, taxa_aprendizado=0.5):
        self.n_entradas = n_entradas
        self.n_oculta = n_oculta or (2 * n_entradas + 1)
        self.n_saida = 1
        self.taxa_aprendizado = taxa_aprendizado

        self.w_entrada_oculta = np.random.uniform(-1, 1, (self.n_entradas, self.n_oculta))
        self.b_oculta = np.zeros((1, self.n_oculta))

        self.w_oculta_saida = np.random.uniform(-1, 1, (self.n_oculta, self.n_saida))
        self.b_saida = np.zeros((1, self.n_saida))

    def forward(self, X):
        self.entrada = X
        self.in_oculta = np.dot(X, self.w_entrada_oculta) + self.b_oculta
        self.out_oculta = sigmoid(self.in_oculta)

        self.in_saida = np.dot(self.out_oculta, self.w_oculta_saida) + self.b_saida
        self.saida = sigmoid(self.in_saida)
        return self.saida

    def backward(self, y):
        erro_saida = y - self.saida
        delta_saida = erro_saida * sigmoid_derivada(self.saida)

        erro_oculta = delta_saida.dot(self.w_oculta_saida.T)
        delta_oculta = erro_oculta * sigmoid_derivada(self.out_oculta)

        self.w_oculta_saida += self.taxa_aprendizado * self.out_oculta.T.dot(delta_saida)
        self.b_saida += self.taxa_aprendizado * np.sum(delta_saida, axis=0, keepdims=True)

        self.w_entrada_oculta += self.taxa_aprendizado * self.entrada.T.dot(delta_oculta)
        self.b_oculta += self.taxa_aprendizado * np.sum(delta_oculta, axis=0, keepdims=True)

        return np.mean(np.abs(erro_saida))

    def treinar(self, X, y, max_iter=10000, tolerancia=1e-4):
        for _ in range(max_iter):
            self.forward(X)
            erro = self.backward(y)
            if erro < tolerancia:
                break

    def prever(self, X):
        return (self.forward(X) > 0.5).astype(int)

def main():
    funcao = input("Escolha a função lógica (AND, OR, XOR): ").strip().upper()
    n = int(input("Digite o número de entradas booleanas (ex: 2, 3, 10): "))

    X, y = gerar_dados(n, funcao)
    rede = MLP(n_entradas=n)
    rede.treinar(X, y)

    print("\nResultados obtidos pela rede MLP com backpropagation:")
    for xi, yi in zip(X, y):
        previsto = rede.prever(np.array([xi]))[0][0]
        print(f"Entrada: {xi}, Esperado: {yi[0]}, Previsto: {previsto}")

if __name__ == '__main__':
    main()

# Célula complementar: implementação de funções de ativação alternativas

import numpy as np

# Função sigmoide (já usada no código principal)
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivada(x):
    return x * (1 - x)

# Função tangente hiperbólica (tanh)
def tanh(x):
    return np.tanh(x)

def tanh_derivada(x):
    return 1 - x ** 2

# Função ReLU (Rectified Linear Unit)
def relu(x):
    return np.maximum(0, x)

def relu_derivada(x):
    return np.where(x > 0, 1, 0)

# Teste simples para visualizar saídas das funções de ativação para alguns valores de entrada
entradas = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])

print("Entradas:", entradas)
print("Sigmoide:", sigmoid(entradas))
print("Tanh:", tanh(entradas))
print("ReLU:", relu(entradas))